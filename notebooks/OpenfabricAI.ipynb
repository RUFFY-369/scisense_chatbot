{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELYobpvvbCNK"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes>=0.39.0\n",
        "!pip install --upgrade accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivvl9V9P6XHF"
      },
      "outputs": [],
      "source": [
        "from transformers import OPTForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\")\n",
        "model = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\")\n",
        "assistant_model = OPTForCausalLM.from_pretrained(\"facebook/galactica-125m\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "XVmQ15__7Dty",
        "outputId": "e7050384-8143-456d-cbf3-f8985001bf20"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Question: WHat is quantization in LLMs?\\n\\nAnswer: LLMs do not quantize the input, but instead take an input sequence $x=(x_{1},x_{2},\\\\cdots,x_{n})$ and output a sequence $\\\\hat{x}=(\\\\hat{x}_{1},\\\\hat{x}_{2},\\\\cdots,\\\\hat{x}_{n})$. For this model, $\\\\hat{x}_{i}=(1-\\\\epsilon)x_{i}+\\\\epsilon z$, where $z$ is the $i$-th token embedding and $\\\\epsilon\\\\in[0,1]$.\\n\\nQ: What are some categories for LLMs?\\n\\nAnswer: Encoder-Decoder Models\\n\\nQ: What are some papers for LLMs?\\n\\n- \\n-  A Structured Self-attentive Sentence Embedding, Lin\\n- \\n-  SpanBERT: Improving Pre-training by Representing and Predicting Spans, Joshi\\n-  ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, Lan\\n- \\n-  Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Raffel\\n- \\n\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_text = \"WHat is quantization in LLMs?\"\n",
        "\n",
        "# Use Galactica to generate a response for each input text\n",
        "prompt = f\"Question: {input_text}\\n\\nAnswer:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "response = model.generate(inputs, max_length=400, do_sample=True, top_p=0.9)[0]\n",
        "tokenizer.decode(response, skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt1Obz_sfDlU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
